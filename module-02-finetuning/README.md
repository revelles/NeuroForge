# Module 02: Fine-Tuning and Model Adaptation

## Goals
- Understand model fine-tuning, parameter-efficient tuning (LoRA/PEFT).
- Use Hugging Face to fine-tune LLMs on custom data.
- Compare zero-shot, few-shot, and fine-tuned behavior.

## Weekly Plan
- Week 5: Tokenization and dataset formatting (Alpaca, JSONL)
- Week 6: Fine-tuning small LLMs with LoRA
- Week 7: Inference + evaluation
- Week 8: Experiment with custom reasoning dataset

## Deliverables
- ðŸ§  Fine-tuned LLM on structured logic or symbolic tasks
- ðŸ“Š Evaluation plots or comparison table
